%% bpmlr.tex
%% V0.1
%% 2015/01/01
%% by Mack Sweeney

\documentclass[10pt]{journal}


% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.


% *** OTHER BIBLIOGRAPHY PACKAGES ***
%
%\usepackage[numbers]{natbib}


% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{./graphics/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}


% *** MATH PACKAGES ***
%
\usepackage[cmex10, fleqn]{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{mathtools}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/


% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
\usepackage{booktabs}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% *** FLOAT PACKAGES ***
%
\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/

% BEGIN PAPER CONTENT
%
\title{Factorization Machine Alternating Least Squares Summary}
\author{
    Mack Sweeney\\
        George Mason University
}
\date{\today}


% New commands to be used in this report.
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\pluseq}{\mathrel{+}=}
\newcommand{\asteq}{\mathrel{*}=}

\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}


\begin{document}
\maketitle


\begin{abstract}

In \cite{rendle_fast_2011}, Rendle et al. derive an efficient alternating least
squares (ALS) algorithm for optimization of the factorization machine (FM). The
goal of the present paper is to lay out this algorithm in increased detail and
create a clear Python implementation which sacrifices performance for ease of
understanding. Increased focus will be placed on notational clarity and
implementation detail.

\end{abstract}


\section{Notation}

The FM model addresses the dyadic response prediction problem, as well as the
matrix completion problem. As such, the model equation and notation can be
formulated in a few different ways. We motivate from the perspective of the
matrix completion problem and then cast our notation into a dyadic response
prediction formulation. The notation will match that of Rendle et al. in
\cite{rendle_fast_2011} where reasonable and deviate only in ways that were
found to increase clarity during implementation. When possible, we also
incorporate notation from \cite{agarwal_regression-based_2009} to reduce mental
load where possible and to attempt to unify some of the notation used throughout
the literature.

Our data consists of a set of $N$ users $U = {u_1, u_2, ..., u_N}$ who rate a
set of $M$ items $I = {i_1, i_2, ..., i_M}$. The ratings are placed in a $U
\times I$ matrix $R$. The average user generally rates only a few of the $M$
items, making $R$ rather sparse in practice. Hence we denote the subset of $R$
which is observed as $S \subset Y$. From the matrix completion perspective, we
define a function $r(i, j)$ that computes $R_{i, j}$, the rating user $i$
assignes to item $j$. The rating prediction task is then to find a function
$\hat{r}(i, j)$ that estimates the rating $R_{i,j}$ for any user-item
combination.

This matrix completion formulation is perfectly adequate for traditional matrix
factorization (MF) techniques such as Singular Value Decomposition (SVD). This is
because these methods do not incorporate side information. Since the FM model
incorporates user and item side information, it is often convenient to cast the
notation of the problem into that of dyadic response prediction. In many problem
settings, it is not actually necessary or desirable to complete the entire $Y$
matrix by predicting all unobserved ratings $A = Y / S$. Instead, we often want
to predict only a subset of the unobserved ratings $P \subset A$ which are of
interest to us. Furthermore, in the matrix completion problem, the individual
users $i$ and items $j$ had only identifiers as relevant information. When
incorporating side information, these scalar IDs are accompanied by other
features and require vectors for storage.

We now require additional notation to represent the feature vectors for users
and items. We denote $\bm{a}_i$ as the feature vector of user $i$ and $\bm{b}_j$
as the feature vector of item $j$. Since we observe user-item dyads $(i, j)$, we
further define a tensor of dyads $\bm{D}^{U \times I \times p}$, where entry
$\bm{D}_{i, j}^{p \times 1} = (\bm{q}_i, \bm{z}_j)$ contains the user-item
features corresponding to the rating $R_{i, j}$. Here $p = |\bm{q}_i| +
|\bm{z}_j|$ is the total number of user-item features. Since we actually only
observe ratings for the dyads corresponding to $S$, we assign each dyad $(i, j)
\in S$ its own unique dyad-specific index $d = {1, ..., O}$ using a mapping
function $id-map: (i, j) \rightarrow d$ subject to $d \in \mathbb{I}, d > 0$.
Since we use this mapping extensively, we define $|S| = O$ to be the number of
observed ratings. So $d = {1, ..., O}$. We place the observed dyads $\bm{D}_{i,
j}: (i, j) \in S$ in a matrix of feature vectors $X^{O \times p}$ and the
observed ratings in a vector $\bm{y}^{O \times 1}$, s.t. $X_d^{p \times 1}$
contains the user-item features for rating $\bm{y}_d$. With this formulation,
the dyadic response prediction task is to discover a function $\hat{y}(X_d)$
that computes an estimate for $y_d$ using the user-item features for dyad $d$.


\section{Factorization Machine Model}

The FM model combines and extends two traditional regression models: linear
regression and MF. Linear regression learns linear interactions between two
variables. It assumes a single linear interaction occurs between each feature
and the target variable. MF models learn linear interactions between three
variables. The assumption is that each user-item pair has a linear effect on the
target variable. Rendle et al. term these two separate types of effects 1-way
interactions and 2-way interactions. The FM model incorporates both of these, as
well as a global bias term which captures the general tendency of the target
variable. One could also consider the global bias term to be equivalent to the
noise term typically incorporated in linear regression and many MF models.

The model equation is shown below. Note that we now revert to using $i$ and $j$
as general indices, rather than as particular users or items. We store the 1-way
interactions in the vector $\bm{w}$ and the 2-way interaction terms in the
matrix $V^{p \times k}$. $k$ is the dimensionality of the matrix factorization.
%
\begin{equation} \label{eq:fm-general}
    \hat{y}(X_d) = w_0 + \sum_{i=1}^p \bm{w}_i X_{d,i} +
        \sum_{i=1}^p \sum_{j=i+1}^p X_{d,i} X_{d,j} \sum_{f=1}^k V_{i,f} V_{j,f}
\end{equation}
%
We can simplify (\ref{eq:fm-general}) slightly by defining a function:
%
\begin{equation} \label{eq:factorize-function}
    f(i, j) = \sum_{f=1}^p V_{i,f} V_{j,f},
\end{equation}
%
which takes feature indices $i, j$ and returns the linear effect for the
interaction of those features. Assume we store these in a matrix $Z^{p \times
p}$, such that $Z_{i,j} = f(i, j)$. Now we can substitute this term into
(\ref{eq:fm-general}):
%
\begin{equation} \label{eq:fm-sub-factorized}
    \hat{y}(X_d) = w_0 + \sum_{i=1}^p \bm{w}_i X_{d,i} +
        \sum_{i=1}^p \sum_{j=i+1}^p X_{d,i} X_{d,j} Z_{i, j}.
\end{equation}
%
Here we can clearly see the various linear effects at play. Naive computation of
this prediction function is $O(kp^2)$, where the $p^2$ term comes from
computation of the factorized interactions $Z$. Rendle et al. showed that
this can be reduced to $O(kp)$ in \cite{rendle_factorization_2010}. We repeat
this here in the notation used above for cohesiveness.

\begin{lemma}
    The model equation of a FM can be computed in linear time $O(kp)$.
\end{lemma}

\begin{proof}
    Due to the factorization of pairwise interactions, there is no model
    parameter that depends directly on two variables (no parameter is indexed by
    both $i$ and $j$). So the pairwise interaction can be reformulated:
%%
    \begin{align}
        & \hphantom{==  } \begin{aligned}[t]
            \sum_{i=1}^p \sum_{j=1}^p X_{d,i} Z_{i,j} X_{d,j}  \\
        \end{aligned} \\
        &= \begin{aligned}[t]
            \frac{1}{2}\sum_{i=1}^p \sum_{j=1}^p \sum_{f=1}^k
                V_{i,f} V_{j,f} X_{d,i} X_{d,j} -
            \frac{1}{2}\sum_{i=1}^p \sum_{f=1}^k V_{i,f} V_{i,f} X_{d,i} X_{d,i}
        \end{aligned} \\
        &= \begin{aligned}[t]
            \frac{1}{2} \sum_{f=1}^k \left(
                \left(\sum_{i=1}^p V_{i,f} X_{d,i}\right)
                \left(\sum_{j=1}^p V_{j,f} X_{d,j}\right) -
                \sum_{i=1}^p V_{i,f}^2 X_{d,i}^2
            \right)
        \end{aligned} \\
        &= \begin{aligned}[t]
            \frac{1}{2} \sum_{f=1}^k \left(
                \left(\sum_{i=1}^p V_{i,f} X_{d,i}\right)^2 -
                \sum_{i=1}^p V_{i,f}^2 X_{d,i}^2
            \right)
        \end{aligned}
    \end{align}
\end{proof}
%
This result gives us the form of the FM equation we intend to optimize:
%
\begin{equation} \label{eq:fm-linear}
    \hat{y}(X_d) = w_0 + \sum_{i=1}^p \bm{w}_i X_{d,i} +
        \frac{1}{2} \sum_{f=1}^k \left(
            \left(\sum_{i=1}^p V_{i,f} X_{d,i}\right)^2 -
            \sum_{i=1}^p V_{i,f}^2 X_{d,i}^2
        \right)
\end{equation}


\section{Alternating Least Squares Algorithm}

There are a variety of ways to optimize the FM model equation defined in
(\ref{eq:fm-linear}). These are summarized in \cite{rendle_factorization_2012}
and include Stochastic Gradient Descent (SGD), SGD with adaptive regularization,
Alternating Least Squares (ALS) aka Coordinate Descent, and Markov Chain Monte
Carlo (MCMC). In this paper, we are interested in the ALS algorithm for
regression. In order to understand this algorithm, we repeat the relevant lemmas
from \cite{rendle_fast_2011}, where the ALS algorithm was originally laid out.

\subsection{Foundation}

First we define $\Theta = {w_0, \bm{w}, V}$, our set of model parameters. Next,
we define the L2-regularized least-squares optimization function:
%
\begin{equation} \label{eq:rls-opt}
    RLS-OPT = \sum_{d=1}^{O} (\hat{y}(X_d) - y_d)^2 +
              \sum_{\theta \in \Theta} \lambda_\theta \theta^2
\end{equation}
%
With the optimization function defined, we can elaborate on the theory that
allows fast optimization.

\begin{lemma}
    An FM is a linear function with respect to every single model parameter
    $\theta \in \Theta$ and thus can be represented as:
%%
    \begin{equation}
        \hat{y}(X_d | \theta) = g_\theta(X_d) + \theta h_\theta(X_d).
    \end{equation}
\end{lemma}

\begin{proof}
    To prove this, we simply state $g$ and $h$ explicitly for each $\theta \in
    \Theta$. We use (\ref{eq:fm-sub-factorized}) for conciseness, but the
    equation in (\ref{eq:fm-linear}) is used during optimization.
%%
    \begin{flalign}
        \hat{y}(X_d | w_0) &= \begin{aligned}[t]
            w_0 \underbrace{(1)}_{h_{(w_0}(X_d)} +
            \underbrace{
                \sum_{i=1}^p \bm{w}_i X_{d,i} +
                \sum_{i=1}^p \sum_{j=i+1}^p Z_{i,j} X_{d,i} X_{d,j}
            }_{g_{(w_0)}(X_d)}
        \end{aligned} \\
%%
        \hat{y}(X_d | \bm{w_l}) &= \begin{aligned}[t]
            \bm{w}_l \underbrace{X_{d,l}}_{h_{(\bm{w}_l)}(X_d)} +
            \underbrace{
                w_0 + \sum_{i=1, i \neq l}^p \bm{w}_i X_{d,i} +
                \sum_{i=1}^p \sum_{j=i+1}^p Z_{i,j} X_{d,i} X_{d,j}
            }_{g_{(\bm{w_l})}(X_d)}
        \end{aligned} \\
%%
        \hat{y}(X_d | V_{l,f}) &= \begin{aligned}[t]
          & V_{l,f} \overbrace{
                X_{d,l} \sum_{i=1, i \neq l}^p V_{i,f} X_{d,i}
            }^{h_{(V_{l,f})}(X_d)} + \\
          & \underbrace{
                w_0 + \sum_{i=1}^p \bm{w}_i X_{d,i} +
                \sum_{i=1}^p \sum_{j=i+1}^p
                \sum_{\mathrlap{\hspace{-2mm}
                    f'=1, (f' \neq f) \cup (l \not\in \{i,j\}) }}^f
                        X_{d,i} X_{d,j} V_{i,f'} V_{j,f'}
            }_{g_{(V_{l,f})}(X_d)}
        \end{aligned}
    \end{flalign}
\end{proof}

\begin{lemma}
    (Optimal value for $\theta$) The regularized least-squares solution of a
    single parameter $\theta$ given all other parameters $\Theta / {\theta}$ for
    a linear model $\hat{y}(X_d| \Theta \ {\theta})$ is:
%%
    \begin{equation}
        \theta = - \frac{ \sum_{d=1}^{O} (g_\theta(X_d) - y_d) h_\theta(X_d) }
                        { \sum_{d=1}^{O} h^2_\theta(X_d) + \lambda_\theta }
    \end{equation}
\end{lemma}

\begin{proof}
    To find the solution analytically, the first derivative of
    (\ref{eq:rls-opt}) w.r.t $\theta$ must be found:
%%
    \begin{equation} \label{eq:rls-opt-derivative}
        \frac{ \partial }{ \partial\theta } \text{RLS-OPT} =
            \sum_{d=1}^{O} 2(\hat{y}(X_d) - y) h_\theta(X_d) + 2\lambda_\theta
    \end{equation}
%%
    The minimum is where this derivative is equal to 0:
%%
    \begin{align}
        & \hphantom{==} \begin{aligned}[t]
            \sum_{d=1}^{O} 2(\hat{y}(X_d) - y) h_\theta(X_d) +
                2\lambda_\theta = 0
        \end{aligned} \\
        & \Leftrightarrow \begin{aligned}[t]
            \sum_{d=1}^{O}
                (g_\theta(X_d) + \theta h_\theta(X_d) - y) h_\theta(X_d) +
            \theta\lambda_\theta = 0
        \end{aligned} \\
        & \Leftrightarrow \begin{aligned}[t]
            \sum_{d=1}^{O}
                (g_\theta(X_d) - y) h_\theta(X_d) +
            \theta\left(
                \sum_{d=1}^{O} h^2_\theta(X_d) + \lambda_\theta
            \right) = 0
        \end{aligned} \\
        & \Leftrightarrow \begin{aligned}[t]
            \theta = -\frac{\sum_{d=1}^{O} (g_\theta(X_d) - y) h_\theta(X_d)}
                           {\sum_{d=1}^{O} h^2_\theta(X_d) + \lambda_\theta}
        \end{aligned}
    \end{align}
\end{proof}

\subsection{Fast Computation}

In order to speed up computation from $\bm{O}(Op^2k)$ to $\bm{O}(Opk)$, we
precompute two caches:
%
\begin{align}
    e_d &= \hat{y_d} - y_d  \\
    q_{d,f} &= \sum_{j=1}^p X_{d,j} V_{j,f}
\end{align}
%
Precomputing the $e$ cache prevents recomputing the error for each individual
dyad during training. Precomputing the $q$ cache allows us to compute the
$h$-terms in constant time using:
%
\begin{equation}
    h_{V_{j,f}}(X_d) = X_{d,j} (q_{d,f} - V_{j,f} X_{d,j})
\end{equation}
%
With these improvements, we now outline the ALS algorithm in
Algorithm~\ref{alg:fm-als}.


\begin{algorithm}
  \caption{FM-ALS($X, y, \lambda_w, \lambda_V, \sigma, \epsilon$)}
  \label{alg:fm-als}
  \begin{algorithmic}[1]
    \State $w0 = 0$  \Comment{Initialize parameters}
    \State $\bm{w} = (0, ..., 0)$
    \State $V \sim \mathcal{N}(0, \sigma)$
    \State $mse = 0$
    \For{$d = \{1, ..., O\}$}  \Comment{Precompute e and q terms}
        \State $e[d] = PREDICT(X[d], w0, w, V) - y[d]$
        \State $mse \pluseq e[d] * e[d]$
        \For{$f = \{1, ..., k\}$}
            \For{$j = \{1, ..., p\}$}
                \State $q[d, f] \pluseq V[j,f] * X[d,j]$
            \EndFor
        \EndFor
    \EndFor
    \State $mse = mse / O$
    \State $prev\_rmse = sqrt(mse)$
    \item[]
%%
    \Repeat  \Comment{Main optimization loop}
        \State $w0\_new = 0$  \Comment{Update global bias}
        \For{$d = \{1, ..., O\}$}
            \State $w0\_new \pluseq e[d] - w_0$
        \EndFor
        \State $w0\_new = w0\_new / O$
        \State $e = e + (w0\_new - w_0)$
        \State $w0 = w0\_new$
        \item[]
%%
        \For{$j = \{1, ..., p\}$}  \Comment{Update 1-way interactions}
            \For{$d = \{1, ..., O\}$}
                \State $err \pluseq e[d] * X_[d,j]$
                \State $x\_sq \pluseq X[d,j] * X[d,j]$
            \EndFor
            \State $w\_new = - (err - w[j] * x\_sq) / (\lambda_w + x\_sq)$
            \State $e = e + (w\_new - w[j])$
            \State $w[j] = w\_new$
        \EndFor
        \item[]
%%
        % Update 2-way interactions
        \For{$f = \{1, ..., k\}$}  \Comment{Update 2-way interactions}
            \For{$j = \{1, ..., p\}$}
                \For{$d = \{1, ..., O\}$}
                    \State $h = X[d,j] * (q[d,f] - X[d,j] * V[j,f])$
                    \State $err \pluseq e[d] * h$
                    \State $h\_sq \pluseq h * h$
                \EndFor
                \State $v\_new = - (err - V[j,f] * h\_sq) / (h\_sq + \lambda_v)$
                \State $e = e + (v\_new - V[j, f])$
                \State $q = q + (v\_new - V[j, f])$
                \State $V[j,f] = v\_new$
            \EndFor
        \EndFor
        \item[]
%%
        % Perform stopping check
        \State $rmse = sqrt(sum(e^2))$  \Comment{Prepare for stopping check}
    \Until ($prev\_rmse - rmse) < \epsilon$
  \end{algorithmic}
\end{algorithm}

\section{Feature Importance}

Inspired by the work of Elbadrawy et al. in \cite{elbadrawy_personalized_2015},
we have derived formulas for computing the importance of each feature. Since the
model does not have non-negativity constraints, we cannot define these
importance metrics simply as the proportion of the overall prediction normalized
over all records. Instead, we seek a quantity inspired by mean absolute
deviation. For each dyad, the active features are involved in one or more
additive interaction terms. The absolute values of these additive terms can be
considered absolute deviations from the global intercept $w_0$. To measure
importance, we sum the additive terms for a particular feature and divide by the
total deviations caused by all terms. So importance is measured as the
proportion of the absolute deviation from the global intercept term accounted
for by a particular feature. Let us consider a particular example to motivate
better understand this idea.

Assume we have three features: a user id, an item id, and a season (Spring,
Summer, Fall, Winter). We one-hot encode all features, so for a particular
record we have three features with a value of 1 and all the rest have values of
0. Now the goal is to predict the response a particular user gives to a
particular item during a particular season. Let us now fixate upon a single
user. This user generally tends to give all items a slightly positive response.
The responses are more positive in the Spring and Fall, and less positive in the
Summer and Winter. Next we fix our sights upon a single item. This item usually
generates very positive responses, with the positivity spiking in the Summer.
The other seasons have no effect on users' responses to the item. The user has
no unusual feelings about the item. We also observe the overall pattern of
responses is slightly positive and the range of ratings is from 0 to 4. Finally,
we do not observe any general seasonality effects. We only find seasonal
preferences of and for particular users and items.

Given this setting, we can now assign hypothetical values to our parameters. We
have 10 users and 20 items, and there are 4 seasons. So after one-hot encoding,
we have a total of 34 features. Let our user be feature 1, our item be feature
11, and the seasons are then the following feature numbers: Spring = 31, Summer
= 32, Fall = 33, Winter = 34. As is typical with regression models, we set
feature 0 to 1 for all records to learn the global intercept term.

\begin{itemize}
    \item  $w_0    = 0.5$, a positive global intercept term.
    \item  $w_1    = 0.5$, a positive reponse trend for this user.
    \item  $w_{11} = 2.0$, a very positive response trend for this item.
    \item  $Z_{1,31} = Z_{1,33} =  0.2$, this user has slightly more positive
        responses in Spring and Fall.
    \item  $Z_{1,32} = Z_{1,34} = -0.2$, this user has slightly less positive
        responses in Summer and Winter.
    \item  $Z_{11,32} = 0.2$, this item has slightly more positive responses in
        the Summer.
\end{itemize}
%
With these parameter values, a prediction for this user-item combo in the Summer
would be calculated as:
%
\begin{align}
    \hat{y}(X_d) \begin{aligned}[t]
         &= w_0 + w_1 + w_{11} + Z_{1,32} + Z_{11,32}  \\
         &= 0.5 + 0.5 + 2.0 - 0.2 + 0.2  \\
         &= 3.0
    \end{aligned}
    \nonumber
\end{align}
%
And the importance of each feature would be calculated as:
%
\begin{equation} \label{eq:importance-single-dyad}
    I(X_{d,l}) = \frac{\sigma_1(X_{d,l}) + \sigma_2(X_{d,l})}{T_d},
\end{equation}
%
where $\sigma_1(X_{d,l})$ is the deviation from 1-way interactions of feature $l$ in
dyad $d$, $\sigma_2(X_{d,l})$ is the deviation from 2-way interactions of feature $l$
in dyad $d$, and $T_d$ is the total deviation from the global intercept $w_0$ in
the estimation for dyad $d$.

\begin{align}
    \sigma_1(X_{d,l})
        &= |w_l X_{d,l}|  \\
    \sigma_2(X_{d,l})
        &= X_{d,l}^2 \sum_{j=1, j \neq l}^p
            \frac{|X_{d,j} Z_{l,j}|}
                 {|X_{d,l}| + |X_{d,j}|}  \\
    T_d
        &= \sum_{j=1}^p |w_j X_{d,j}| +
            \sum_{j=1}^p \sum_{j'=j+1}^p
                |X_{d,j} X_{d,j'} Z_{j,j'}|
\end{align}
%
Recall that we have three features set to 1 for each dyad feature vector $X_d$.
So we calculate the importance of these three features using
(\ref{eq:importance-single-dyad}).
%
\begin{align*}
    T_d
        &= |w_1| + |w_{11}| + |Z_{1,32}| + |Z_{11,32}|  \\
        &= |0.5| + |2.0| + |-0.2| + |0.2| = 2.9  \\[3mm]
%%
    I(X_{d,1})
        &= \frac{|w_1| + |Z_{1,32}| / 2}{T_d}  \\
        &= \frac{|0.5| + |-0.2| / 2}{2.9}
        = \frac{0.6}{2.9} \approx 0.2069  \\[3mm]
%%
    I(X_{d,11})
        &= \frac{|w_{11}| + |Z_{11,32}| / 2}{T_d}  \\
        &= \frac{|2.0| + |0.2| / 2}{2.9}
        = \frac{2.1}{2.9} \approx 0.7241  \\[3mm]
%%
    I(X_{d,32})
        &= \frac{|Z_{1,32}| / 2 + |Z_{11,32}| / 2}{T_d}  \\
        &= \frac{|-0.2| / 2 + |0.2| / 2}{2.9}
         = \frac{0.2}{2.9} \approx 0.0690  \\
\end{align*}

Having worked through this example, we now make several observations about the
properties of the importance metrics defined so far.

\begin{itemize}
    \item  $\sum_{j=1}^p I(X_{d,j}) = T_d$. The sum of the importance measures
        of all features for a dyad equals the total deviation for that dyad.
    \item  We can calculate the overall importance of a feature in our training
        dataset through:
        \begin{equation}
            I(X_{*,j}) = \sum_{d=1}^O I(X_{d,j})
        \end{equation}
    \item  Calculation of the overall importance measures for all features is a
        \sloppy{$\bm{O}(O(p^2 + k))$} operation if we precompute the $Z$ terms.
\end{itemize}


\subsection{Efficient Importance Computation}

The anticipated use of feature importance is qualitative analysis in an offline
setting. So the polynomial complexity is not especially troubling and heuristics
for estimation of these quantities will not be devised at this time. However, it
is still advantageous to think ahead towards an efficient vectorized procedure
for computing feature importance. We can break down the computation into
computation of the:

\begin{itemize}
    \item  $Z$ terms,
    \item  $\sigma_1$ 1-way interaction deviations,
    \item  $\sigma_2$ 2-way interaction deviations,
    \item  and the $T_d$ total absolute deviations.
\end{itemize}

The first computation is a simple dot product: $Z^{p \times p} = V V^T$.
Computation of $\sigma_1$ requires multiplying the vector $\bm{w}^{p \times 1}$
by each row of the training data $X^{O \times p}$. If using a dense matrix
representation for $X$, this is often trivial. In Python's `numpy` library, it
can be handled by a single multiplication operation with the help of numpy's
broadcasting. However, if a sparse representation such as compressed storage row
(CSR) format is being used, the operation will likely require looping over the
rows.

Computation of $\sigma_2$ is more involved. We can vectorize it if we
reformulate slightly:
%
\begin{align}
    \sigma_2(X_{d,l})
        &= X_{d,l}^2 \sum_{j=1, j \neq l}^p
            \frac{|X_{d,j} Z_{l,j}|}
                 {|X_{d,l}| + |X_{d,j}|}  \\
        &= X_{d,l}^2 \sum_{j=1}^p
            \frac{|X_{d,j} Z_{l,j}|}
                 {|X_{d,l}| + |X_{d,j}|} -
             X_{d,l}^2 \frac{|X_{d,l} Z_{l,l}|}
                            {|X_{d,l}| + |X_{d,l}|}  \\
        &= X_{d,l}^2 \left(
            \sum_{j=1}^p
                \frac{|X_{d,j} Z_{l,j}|}
                     {|X_{d,l}| + |X_{d,j}|} -
            \frac{|Z_{l,l}|}{2}
        \right) \\
        &= X_{d,l}^2 \left(  \label{eq:sigma_2}
            sum \left(
                \frac{|X_{d,*} * Z_{l,*}|^{j \times 1}}
                {(|X_{d,l}| + |X_{d,*}|)^{j \times 1}}
            \right) - \frac{|Z_{l,l}|}{2}
        \right)
\end{align}
%
Working from (\ref{eq:sigma_2}), we can now compute the inner loop over our $p$
features using two vectorized operations. Excluding the absolute value
operations for the moment, we see the numerator is a multiplication of
two vectors: $X_{d,*} * Z_{l, *}$ and the denominator is a scalar plus a vector:
$X_{d,l} + X_{d,*}$. Afterwards we have an elementwise division of the two
resulting vectors and a sum of all elements. All of these elementwise operations
can be performed efficiently using GSL, BLAS, or some other scientific
computation library.

Finally, once we have computed both $\sigma_1$ and $\sigma_2$ for every record,
the total deviation terms can be computed by adding them. Assume we store the
resulting values in $O \times p$ matrices $\Sigma_1$ and $\Sigma_2$. We use
vectorized row sum operations followed by elementwise addition. Finally, since
our goal is to compute overall importance metrics rather than per-dyad
importance, we sum the entries of $T$.
%
\begin{align}
    T_d &= \sum_{j=1}^p \sigma_1(X_{d,j}) + \sigma_2(X_{d,j})  \\[3mm]
    T   &= rowsum(\Sigma_1) + rowsum(\Sigma_2)  \\[3mm]
    T^* &= sum(T)
\end{align}
%
Individual feature importance can now be computed with:
%
\begin{align}
    I(j) &= \frac{\Sigma_1[:, j] + \Sigma_2[:, j]}{T*}  \\[3mm]
    I    &= \frac{colsum(\Sigma_1) + colsum(\Sigma_2)}{T*}
\end{align}
%

Having explored the pieces of our feature importance computation, we present FM
feature importance (FM-FI) algorithm in Algorithm~\ref{alg:fm-fi}. We store the
feature importances in a vector $\bm{f}$. To avoid repeated operations, we store
a copy of X and Z with absolute values already computed.

\begin{algorithm}
    \caption{FM-FI($X, \bm{w}, V$)}
    \label{alg:fm-fi}
    \begin{algorithmic}[1]
        \State $X = |X|$  \Comment{Take absolutes and init memory}
        \State $\bm{w} = |\bm{w}|$
        \State $\bm{f} = \bm{0}^{j x 1}$
        \State $\Sigma_1 = \bm{0}^{O \times p}$
        \State $\Sigma_2 = \bm{0}^{O \times p}$
        \State $Z = |V \cdot V^T|$  \Comment{Precompute $Z$ terms}
        \item[]
%%
        \For{$d = \{1, ..., O\}$}  \Comment{Compute $\sigma_1$ for all $j$}
            \State $\Sigma_1[d,:] = X[d,:] * \bm{w}$
        \EndFor
        \item[]
%%
        \For{$d = \{1, ..., O\}$}  \Comment{Compute $\sigma_2$ for all $j$}
            \For{$j = \{1, ..., p\}$}
                \State $\bm{t} = X[d,:] * Z[j,:]$
                \State $\bm{b} = X[d,j] + X[d,:]$
                \State $\Sigma_2[d,j] = X[d,j]^2
                    (sum(\frac{\bm{t}}{\bm{b}}) - \frac{Z[j,j]}{2})$
            \EndFor
        \EndFor
        \item[]
%%
        \State $T = rowsum(\Sigma_1) + rowsum(\Sigma_2)$
            \Comment{Compute $T_d$ terms and $f$}
        \State $f = \frac{colsum(\Sigma_1) + colsum(\Sigma_2)}{sum(T)}$

    \end{algorithmic}
\end{algorithm}


%%
%% BIBLIOGRAPHY
%%
\bibliographystyle{siam}
\bibliography{refs}


\end{document}

